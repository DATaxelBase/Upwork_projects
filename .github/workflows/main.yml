name: Scrape

on:
  schedule:
    - cron: "0 12 * * *"
  workflow_dispatch:

env:
  ACTIONS_ALLOW_UNSECURE_COMMANDS: true
  type: ${{secrets.TYPE}}
  project_id: ${{secrets.PROJECT_ID}}
  private_key_id: ${{secrets.PRIVATE_KEY_ID}}
  private_key: ${{secrets.PRIVATE_KEY}}
  client_email: ${{secrets.CLIENT_EMAIL}}
  client_id: ${{secrets.CLIENT_ID}}
  auth_uri: ${{secrets.AUTH_URI}}
  token_uri: ${{secrets.TOKEN_URI}}
  auth_provider_x509_cert_url: ${{secrets.AUTH_PROVIDER_X509_CERT_URL}} 
  client_x509_cert_url: ${{secrets.CLIENT_X509_CERT_URL}}
  universe_domain: ${{secrets.UNIVERSE_DOMAIN}}


jobs:
  scrape-latest:
    runs-on: ubuntu-latest

    steps:
      - name: Upgrade pip
        run: |
          python -m pip install --upgrade pip

      - name: Checkout repository
        uses: actions/checkout@v4 # Use the latest version compatible
        

      - name: Set up Python
        uses: actions/setup-python@v4 # Use the latest version compatible
        with:
          python-version: 3.9
      - name: Installing package list
        run: apt list --installed    
      - name: Removing previous chrome instances on runner 
        run: sudo apt purge google-chrome-stable
       # Need to fetch reqs if needed
      - name: Installing all necessary packages
        run: |
          pip install chromedriver-autoinstaller selenium pyvirtualdisplay
          pip install --upgrade google-api-python-client oauth2client
      - name: Install xvfb
        run: sudo apt-get install xvfb
      - name: Run Scraper
        run: |
          python -m pip install -r .github/workflows/requirements.txt 
          python max_pain_scraping.py


